#!/usr/bin/env python3
"""
Content Extractor Service - File Processing & Google Drive Integration

ðŸ“‹ PIPELINE REFERENCE: See docs/EMAIL_PROCESSING_PIPELINE.md for complete architecture

ðŸŽ¯ SERVICE PURPOSE & ARCHITECTURAL ROLE:
This service handles BINARY FILE PROCESSING ONLY - converting files that cannot be 
read as plain text into readable content. It does NOT handle text normalization.

âš ï¸ CRITICAL ARCHITECTURAL BOUNDARY:
=====================================

âœ… CONTENT EXTRACTOR HANDLES:
- Binary file conversion (PDF â†’ text, Word â†’ text, Images â†’ OCR text)
- Drive upload/download coordination via Email Processor
- Background job processing for Gmail attachment automation
- File format detection and metadata extraction
- Manual file processing API endpoints

âŒ CONTENT EXTRACTOR DOES NOT HANDLE:
- HTML â†’ markdown conversion (Pre-Processor handles this)
- Plain text â†’ formatted markdown (Pre-Processor handles this)  
- YAML front-matter injection (Pre-Processor handles this)
- Text chunking and language detection (Pre-Processor handles this)

ðŸ”— INTEGRATION PATTERN:
======================
Binary Files â†’ Content Extractor (converts to raw text) â†’ Pre-Processor (normalizes text) â†’ AI Processor

Example Flow:
1. PDF attachment received via Gmail
2. Content Extractor: PDF â†’ raw text extraction
3. Content Extractor calls Pre-Processor: raw text â†’ markdown with metadata
4. Pre-Processor returns formatted content for AI analysis

ðŸš¨ DEVELOPER WARNING:
====================
DO NOT add markdownify, HTML processing, or text normalization to this service!
Use the Pre-Processor service API for ALL text processing needs.

Integration Example:
```python
# âœ… CORRECT - Use Pre-Processor for text normalization
normalized_result = await self.pre_processor_client.normalize_content(
    raw_text, idea_id, source
)

# âŒ WRONG - Don't duplicate Pre-Processor functionality
from markdownify import markdownify  # Never import this here!
```

ðŸ“¡ API ENDPOINTS:
================
- GET /health - Service health check
- POST /manual/process-file - User-triggered file conversion
- POST /manual/process-text - User-triggered text processing (delegates to Pre-Processor)
- Background: Automatic Gmail attachment processing

ðŸ”§ TECHNICAL STACK:
==================
- PyPDF2: PDF text extraction
- python-docx: Word document processing  
- pytesseract: Image OCR processing
- python-magic: File type detection
- httpx: Pre-Processor service integration
- asyncpg: Background job processing

Last Updated: 2025-07-17
Status: âœ… Production Ready with proper service boundaries
"""

import os
import asyncio
import structlog
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException, UploadFile, File, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, HttpUrl
from typing import List, Dict, Any, Optional
import uvicorn
import aiofiles
import tempfile
from pathlib import Path
import asyncpg
import httpx

# Import our file converter
from src.file_converter import FileConverter

# Configure structured logging
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger(__name__)

# Global variables for background tasks
conversion_job_processor = None
polling_task = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage application lifespan with proper startup/shutdown"""
    global conversion_job_processor, polling_task
    
    # Startup
    conversion_job_processor = ConversionJobProcessor()
    await conversion_job_processor.start()
    
    # Start background polling task
    polling_task = asyncio.create_task(conversion_job_processor.poll_and_process())
    logger.info("Background conversion job polling started")
    
    yield
    
    # Shutdown
    if conversion_job_processor:
        await conversion_job_processor.stop()
    
    if polling_task:
        polling_task.cancel()
        try:
            await polling_task
        except asyncio.CancelledError:
            pass
    
    logger.info("Content Extractor service shut down")

# Initialize FastAPI app with lifespan
app = FastAPI(
    title="Content Extractor Service",
    description="URL content extraction and file parsing for Ideas Database",
    version="1.0.0",
    lifespan=lifespan
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure appropriately for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Pydantic models
class URLExtractionRequest(BaseModel):
    url: HttpUrl
    request_id: str
    extract_images: bool = False
    extract_links: bool = True
    max_depth: int = 1

class FileExtractionRequest(BaseModel):
    file_id: str
    file_type: str
    extract_metadata: bool = True

class ExtractionResult(BaseModel):
    request_id: str
    content_type: str
    title: Optional[str] = None
    text_content: str
    metadata: Dict[str, Any]
    images: List[str] = []
    links: List[str] = []
    extraction_status: str
    processing_time: float

class HealthResponse(BaseModel):
    status: str
    service: str
    version: str

class PreProcessorClient:
    """Client for communicating with Pre-Processor service"""
    
    def __init__(self):
        self.pre_processor_host = os.getenv('PRE_PROCESSOR_HOST', 'idea_db_pre_processor')
        self.base_url = f"http://{self.pre_processor_host}:8000"
    
    async def normalize_content(self, content: str, idea_id: str, source: str) -> Dict[str, Any]:
        """Send content to pre-processor for normalization"""
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    f"{self.base_url}/normalize",
                    json={
                        "idea_id": idea_id,
                        "source": source,
                        "payload": content
                    }
                )
                
                if response.status_code == 200:
                    return response.json()
                else:
                    raise Exception(f"Pre-processor normalize failed: {response.status_code} - {response.text}")
                    
        except Exception as e:
            logger.error("Failed to normalize content with pre-processor", error=str(e))
            raise

class ConversionJobProcessor:
    """Processes conversion jobs for automatic Drive uploads"""
    
    def __init__(self):
        self.postgres_url = os.getenv('POSTGRES_URL')
        self.email_processor_host = os.getenv('EMAIL_PROCESSOR_HOST', 'idea_db_email_processor')
        self.connection_pool = None
        self.is_running = False
        self.file_converter = FileConverter()
        self.pre_processor_client = PreProcessorClient()
        
    async def start(self):
        """Initialize database connection and start processing"""
        try:
            self.connection_pool = await asyncpg.create_pool(self.postgres_url)
            self.is_running = True
            logger.info("ConversionJobProcessor started")
        except Exception as e:
            logger.error("Failed to start ConversionJobProcessor", error=str(e))
            raise
    
    async def stop(self):
        """Stop processing and close connections"""
        self.is_running = False
        if self.connection_pool:
            await self.connection_pool.close()
        logger.info("ConversionJobProcessor stopped")
    
    async def poll_and_process(self):
        """Main polling loop - checks for pending conversion jobs every 10 seconds"""
        while self.is_running:
            try:
                await self.process_pending_jobs()
                await asyncio.sleep(10)  # Poll every 10 seconds
            except Exception as e:
                logger.error("Error in polling loop", error=str(e))
                await asyncio.sleep(30)  # Wait longer on error
    
    async def process_pending_jobs(self):
        """Process all pending conversion jobs"""
        if not self.connection_pool:
            return
            
        async with self.connection_pool.acquire() as conn:
            # Get pending jobs ordered by priority and creation time
            jobs = await conn.fetch("""
                SELECT id, attachment_id, job_type, retry_count
                FROM idea_database.conversion_jobs 
                WHERE status = 'pending' 
                ORDER BY priority ASC, created_at ASC
                LIMIT 5
            """)
            
            for job in jobs:
                try:
                    await self.process_job(job, conn)
                except Exception as e:
                    logger.error("Failed to process job", job_id=str(job['id']), error=str(e))
                    await self.mark_job_failed(job['id'], str(e), conn)
    
    async def process_job(self, job, conn):
        """Process a single conversion job"""
        job_id = job['id']
        attachment_id = job['attachment_id']
        
        logger.info("Processing conversion job", job_id=str(job_id), attachment_id=str(attachment_id))
        
        # Mark job as processing
        await conn.execute("""
            UPDATE idea_database.conversion_jobs 
            SET status = 'processing', started_at = NOW()
            WHERE id = $1
        """, job_id)
        
        # Get attachment details
        attachment = await conn.fetchrow("""
            SELECT gmail_message_id, gmail_attachment_id, filename, file_type, source_email_id
            FROM idea_database.attachments 
            WHERE id = $1
        """, attachment_id)
        
        if not attachment:
            raise Exception(f"Attachment {attachment_id} not found")
        
        # Download attachment from Gmail and upload to Drive
        result = await self.download_and_upload_to_drive(attachment)
        
        # Update attachment with Drive information
        if result:
            await conn.execute("""
                UPDATE idea_database.attachments 
                SET drive_file_id = $1, drive_file_url = $2, storage_type = 'drive'
                WHERE id = $3
            """, result.get('file_id'), result.get('file_url'), attachment_id)
            
            # Optional: Process file for markdown conversion if needed for AI analysis
            # This creates the processed markdown but doesn't interfere with original Drive storage
            try:
                await self.process_file_for_ai_analysis(attachment, result, conn)
            except Exception as e:
                logger.warning("File processing for AI analysis failed", 
                              attachment_id=str(attachment_id), error=str(e))
                # Don't fail the main job for optional AI processing
        
        # Mark job as completed
        await conn.execute("""
            UPDATE idea_database.conversion_jobs 
            SET status = 'completed', completed_at = NOW()
            WHERE id = $1
        """, job_id)
        
        logger.info("Conversion job completed", job_id=str(job_id))
    
    async def download_and_upload_to_drive(self, attachment):
        """Download attachment from Gmail and upload to Drive"""
        try:
            # Call email processor to download from Gmail and upload to Drive
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    f"http://{self.email_processor_host}:8000/gmail/attachment/download-and-upload",
                    json={
                        "message_id": attachment['gmail_message_id'],
                        "attachment_id": attachment['gmail_attachment_id'],
                        "filename": attachment['filename']
                    }
                )
                
                if response.status_code == 200:
                    return response.json()
                else:
                    raise Exception(f"Upload failed: {response.status_code} - {response.text}")
                    
        except Exception as e:
            logger.error("Failed to download and upload attachment", 
                        attachment_id=attachment['gmail_attachment_id'], error=str(e))
            raise
    
    async def process_file_for_ai_analysis(self, attachment, drive_result, conn):
        """Optional: Convert file to markdown for AI analysis without affecting Drive storage"""
        try:
            # Download file from Drive for processing
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.get(
                    f"http://{self.email_processor_host}:8000/gmail/attachment/download",
                    params={
                        "message_id": attachment['gmail_message_id'],
                        "attachment_id": attachment['gmail_attachment_id']
                    }
                )
                
                if response.status_code != 200:
                    raise Exception(f"Failed to download file for processing: {response.status_code}")
                
                file_content = response.content
            
            # Convert file to markdown using our file converter
            conversion_result = await self.file_converter.convert_to_markdown(
                file_content, 
                attachment['filename'], 
                attachment['file_type']
            )
            
            if not conversion_result['success']:
                raise Exception(f"File conversion failed: {conversion_result.get('error', 'Unknown error')}")
            
            markdown_content = ""
            
            # Check if this requires pre-processing (text files)
            if conversion_result.get('requires_preprocessing'):
                # Send to pre-processor for normalization
                idea_id = f"email_{attachment['source_email_id']}_attachment_{attachment['gmail_attachment_id']}"
                normalized_result = await self.pre_processor_client.normalize_content(
                    conversion_result['raw_content'], 
                    idea_id, 
                    f"email_attachment:{attachment['filename']}"
                )
                markdown_content = normalized_result.get('markdown', '')
            else:
                # Binary file already converted to markdown
                markdown_content = conversion_result['markdown_content']
            
            # Store processed markdown in database for AI analysis (optional table)
            # This doesn't replace the original Drive file, just adds processed content
            if markdown_content:
                await conn.execute("""
                    INSERT INTO idea_database.processed_content 
                    (attachment_id, markdown_content, processing_metadata, created_at)
                    VALUES ($1, $2, $3, NOW())
                    ON CONFLICT (attachment_id) DO UPDATE SET
                        markdown_content = EXCLUDED.markdown_content,
                        processing_metadata = EXCLUDED.processing_metadata,
                        updated_at = NOW()
                """, 
                attachment['id'] if 'id' in attachment else None,
                markdown_content,
                {
                    'conversion_metadata': conversion_result.get('metadata', {}),
                    'processing_type': 'pre_processor_normalized' if conversion_result.get('requires_preprocessing') else 'binary_converted',
                    'original_filename': attachment['filename']
                })
                
                logger.info("File processed for AI analysis", 
                           filename=attachment['filename'], 
                           content_length=len(markdown_content))
                           
        except Exception as e:
            logger.error("AI processing failed for attachment", 
                        filename=attachment['filename'], error=str(e))
            raise

    async def mark_job_failed(self, job_id, error_message, conn):
        """Mark job as failed and increment retry count"""
        await conn.execute("""
            UPDATE idea_database.conversion_jobs 
            SET status = 'failed', error_message = $1, retry_count = retry_count + 1
            WHERE id = $2
        """, error_message, job_id)

# Root endpoint
@app.get("/")
async def root():
    """Root endpoint with service information"""
    return {
        "service": "content_extractor",
        "version": "1.0.0",
        "description": "URL content extraction, file parsing, and markdown processing for Ideas Database",
        "endpoints": {
            "health": "/health",
            "extract_url": "/extract/url",
            "extract_file": "/extract/file",
            "batch_urls": "/extract/batch/urls",
            "status": "/status/{request_id}",
            "supported_types": "/supported-types",
            "manual_process_file": "/manual/process-file",
            "manual_process_text": "/manual/process-text"
        },
        "features": {
            "automatic_processing": "Gmail attachments â†’ Drive upload with optional AI processing",
            "manual_processing": "User-triggered file/text â†’ markdown conversion for AI analysis",
            "pre_processor_integration": "Seamless integration with existing pre-processor service"
        },
        "supported_formats": ["PDF", "DOCX", "TXT", "HTML", "MD", "CSV", "Images (OCR)"]
    }

# Health check endpoint
@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint"""
    return HealthResponse(
        status="healthy",
        service="content_extractor",
        version="1.0.0"
    )

# Extract content from URL
@app.post("/extract/url", response_model=ExtractionResult)
async def extract_url_content(request: URLExtractionRequest, background_tasks: BackgroundTasks):
    """Extract content from a URL"""
    logger.info("Extracting URL content", url=str(request.url), request_id=request.request_id)
    
    try:
        # Placeholder for URL extraction logic
        # TODO: Implement actual URL content extraction
        result = ExtractionResult(
            request_id=request.request_id,
            content_type="webpage",
            title="Sample Article Title",
            text_content="This is extracted text content from the URL...",
            metadata={
                "url": str(request.url),
                "domain": request.url.host,
                "word_count": 150,
                "language": "en",
                "extracted_at": "2025-06-22T14:00:00Z"
            },
            images=["https://example.com/image1.jpg"] if request.extract_images else [],
            links=["https://example.com/link1", "https://example.com/link2"] if request.extract_links else [],
            extraction_status="completed",
            processing_time=1.2
        )
        
        logger.info("URL content extracted successfully", request_id=request.request_id)
        return result
        
    except Exception as e:
        logger.error("Error extracting URL content", request_id=request.request_id, error=str(e))
        raise HTTPException(status_code=500, detail=f"URL extraction failed: {str(e)}")

# Upload and extract file content
@app.post("/extract/file", response_model=ExtractionResult)
async def extract_file_content(
    file: UploadFile = File(...),
    request_id: str = "default",
    extract_metadata: bool = True
):
    """Extract content from uploaded file"""
    logger.info("Extracting file content", filename=file.filename, request_id=request_id)
    
    try:
        # Create temporary file
        with tempfile.NamedTemporaryFile(delete=False, suffix=Path(file.filename).suffix) as temp_file:
            content = await file.read()
            temp_file.write(content)
            temp_file_path = temp_file.name
        
        try:
            # Placeholder for file extraction logic
            # TODO: Implement actual file content extraction
            result = ExtractionResult(
                request_id=request_id,
                content_type=file.content_type or "unknown",
                title=file.filename,
                text_content="This is extracted text content from the uploaded file...",
                metadata={
                    "filename": file.filename,
                    "file_size": len(content),
                    "content_type": file.content_type,
                    "extracted_at": "2025-06-22T14:00:00Z"
                },
                extraction_status="completed",
                processing_time=0.8
            )
            
            logger.info("File content extracted successfully", request_id=request_id)
            return result
            
        finally:
            # Clean up temporary file
            os.unlink(temp_file_path)
        
    except Exception as e:
        logger.error("Error extracting file content", request_id=request_id, error=str(e))
        raise HTTPException(status_code=500, detail=f"File extraction failed: {str(e)}")

# Batch URL extraction
@app.post("/extract/batch/urls")
async def extract_batch_urls(urls: List[URLExtractionRequest], background_tasks: BackgroundTasks):
    """Extract content from multiple URLs"""
    logger.info("Processing URL batch", count=len(urls))
    
    try:
        results = []
        for url_request in urls:
            # Add to background tasks for async processing
            background_tasks.add_task(extract_url_background, url_request)
            results.append({"request_id": url_request.request_id, "status": "queued"})
        
        return {"processed": len(urls), "results": results}
        
    except Exception as e:
        logger.error("Error processing URL batch", error=str(e))
        raise HTTPException(status_code=500, detail=f"Batch URL extraction failed: {str(e)}")

async def extract_url_background(request: URLExtractionRequest):
    """Background task for URL extraction"""
    logger.info("Background URL extraction", request_id=request.request_id)
    # TODO: Implement background URL extraction logic
    await asyncio.sleep(2)  # Placeholder
    logger.info("Background URL extraction completed", request_id=request.request_id)

# Get extraction status
@app.get("/status/{request_id}")
async def get_extraction_status(request_id: str):
    """Get extraction status for a specific request"""
    # TODO: Implement status tracking
    return {"request_id": request_id, "status": "completed", "timestamp": "2025-06-22T14:00:00Z"}

# List supported file types
@app.get("/supported-types")
async def get_supported_types():
    """Get list of supported file types for extraction"""
    return {
        "document_types": ["pdf", "docx", "txt", "md", "rtf"],
        "spreadsheet_types": ["xlsx", "csv", "ods"],
        "presentation_types": ["pptx", "odp"],
        "image_types": ["jpg", "jpeg", "png", "gif", "bmp", "tiff"],
        "web_types": ["html", "xml", "json"],
        "archive_types": ["zip", "tar", "gz"]
    }

# Manual markdown processing endpoints
@app.post("/manual/process-file")
async def manual_process_file(
    file: UploadFile = File(...),
    idea_id: str = "manual",
    source: str = "manual_upload"
):
    """Manually process uploaded file to markdown for AI analysis"""
    logger.info("Manual file processing", filename=file.filename, idea_id=idea_id)
    
    try:
        # Read file content
        file_content = await file.read()
        
        # Initialize file converter
        file_converter = FileConverter()
        
        # Convert to markdown
        conversion_result = await file_converter.convert_to_markdown(
            file_content, file.filename, file.content_type
        )
        
        if not conversion_result['success']:
            raise HTTPException(status_code=400, detail=conversion_result.get('error', 'Conversion failed'))
        
        # Check if this requires pre-processing (text files)
        if conversion_result.get('requires_preprocessing'):
            # Send to pre-processor for normalization
            pre_processor_client = PreProcessorClient()
            normalized_result = await pre_processor_client.normalize_content(
                conversion_result['raw_content'], 
                idea_id, 
                source
            )
            
            return {
                "success": True,
                "processing_type": "pre_processor_normalized",
                "markdown_content": normalized_result.get('markdown', ''),
                "metadata": conversion_result['metadata']
            }
        else:
            # Binary file already converted to markdown
            return {
                "success": True,
                "processing_type": "binary_converted",
                "markdown_content": conversion_result['markdown_content'],
                "metadata": conversion_result['metadata']
            }
            
    except Exception as e:
        logger.error("Manual file processing failed", filename=file.filename, error=str(e))
        raise HTTPException(status_code=500, detail=f"File processing failed: {str(e)}")

@app.post("/manual/process-text")
async def manual_process_text(
    content: str,
    idea_id: str = "manual",
    source: str = "manual_text",
    content_type: str = "text/plain"
):
    """Manually process text content through pre-processor for AI analysis"""
    logger.info("Manual text processing", idea_id=idea_id, content_length=len(content))
    
    try:
        # Send directly to pre-processor for normalization
        pre_processor_client = PreProcessorClient()
        normalized_result = await pre_processor_client.normalize_content(content, idea_id, source)
        
        return {
            "success": True,
            "processing_type": "text_normalized",
            "markdown_content": normalized_result.get('markdown', ''),
            "metadata": {
                "original_content_length": len(content),
                "content_type": content_type,
                "source": source
            }
        }
        
    except Exception as e:
        logger.error("Manual text processing failed", idea_id=idea_id, error=str(e))
        raise HTTPException(status_code=500, detail=f"Text processing failed: {str(e)}")

if __name__ == "__main__":
    port = int(os.getenv("CONTENT_EXTRACTOR_PORT", 8000))
    logger.info("Starting Content Extractor Service", port=port)
    
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=port,
        reload=False,
        log_config=None  # Use structlog instead
    ) 