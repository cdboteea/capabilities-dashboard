# Pre-Processor Service: Text Normalization & Chunking

## üéØ Service Purpose & Architecture Role

The Pre-Processor service is responsible for **ALL TEXT NORMALIZATION** in the AI Research Platform. It handles conversion of text content (HTML, plain text, etc.) to standardized markdown format with YAML front-matter and semantic chunking for LLM processing.

### ‚ö†Ô∏è CRITICAL ARCHITECTURAL BOUNDARY

**Pre-Processor handles:** 
- ‚úÖ HTML ‚Üí markdown conversion using markdownify
- ‚úÖ Plain text ‚Üí formatted markdown  
- ‚úÖ YAML front-matter injection (idea_id, source, timestamps)
- ‚úÖ Semantic chunking (by headings, paragraphs, with overlap)
- ‚úÖ Language detection and metadata enrichment
- ‚úÖ Deduplication by content hash

**Pre-Processor does NOT handle:**
- ‚ùå Binary file processing (PDF, Word, Images) ‚Üí Content Extractor handles this
- ‚ùå Drive upload/download ‚Üí Email Processor handles this
- ‚ùå File storage management ‚Üí Other services handle this

### üîó Service Integration Pattern

```
Email Content ‚Üí Pre-Processor ‚Üí AI Processor
Binary Files ‚Üí Content Extractor ‚Üí Pre-Processor ‚Üí AI Processor
Manual Text ‚Üí Content Extractor ‚Üí Pre-Processor ‚Üí AI Processor
```

**DO NOT duplicate HTML/text processing in other services!**

## ‚úÖ Current Implementation Status (as of 2025-07-17)

- **Service:** FastAPI, Dockerized, port 3006:8000, on ai_platform network
- **Status:** ‚úÖ PRODUCTION READY with full functionality
- **Endpoints:** All operational with complete feature set
- **Integration:** Successfully integrated with Content Extractor service
- **Testing:** Verified working with live content processing

### üöÄ Implemented Features

#### 1. Text Normalization (`/normalize`)
- ‚úÖ HTML ‚Üí clean markdown conversion using markdownify
- ‚úÖ Plain text ‚Üí formatted markdown
- ‚úÖ YAML front-matter injection with metadata
- ‚úÖ Input validation and size limits (1MB max)
- ‚úÖ Error handling for malformed content
- ‚úÖ Redis event publishing for downstream processing

#### 2. Semantic Chunking (`/chunk`)
- ‚úÖ Intelligent splitting by headings (H2/H3 sections)
- ‚úÖ Paragraph-based fallback chunking
- ‚úÖ Configurable chunk size (4000 chars max, 400 chars overlap)
- ‚úÖ Deduplication by SHA1 hash
- ‚úÖ Language detection per chunk
- ‚úÖ Token counting and metadata enrichment
- ‚úÖ YAML front-matter removal before chunking

#### 3. Service Integration
- ‚úÖ Content Extractor integration via HTTP API
- ‚úÖ Redis event publishing for AI Processor
- ‚úÖ Async request handling with FastAPI
- ‚úÖ Comprehensive error handling and logging

## üì° API Endpoints

### POST `/normalize`
Converts text content to standardized markdown with YAML front-matter.

**Request:**
```json
{
  "idea_id": "string",
  "source": "string", 
  "payload": "string (HTML or plain text)"
}
```

**Response:**
```json
{
  "markdown": "---\nidea_id: xyz\nsource: email\ncreated_at: 2025-07-17T...\n---\n\n# Content..."
}
```

### POST `/chunk`
Splits markdown into semantic chunks for LLM processing.

**Request:**
```json
{
  "idea_id": "string",
  "markdown": "string (markdown with YAML front-matter)"
}
```

**Response:**
```json
[
  {
    "chunk_id": "string",
    "idea_id": "string", 
    "order": 0,
    "text": "string",
    "token_count": 150,
    "lang": "en",
    "hash": "sha1_hash",
    "created_at": "2025-07-17T..."
  }
]
```

### GET `/health`
Service health check.

## üõ†Ô∏è Technical Implementation

### Dependencies
- FastAPI for async HTTP API
- markdownify for HTML‚Üímarkdown conversion
- PyYAML for front-matter injection
- langdetect for language identification  
- redis for event publishing
- pydantic for request validation

### Processing Pipeline
1. **Input Validation** - Size limits, content type validation
2. **Content Detection** - HTML vs plain text identification
3. **Normalization** - markdownify conversion or text formatting
4. **Metadata Injection** - YAML front-matter with timestamps
5. **Event Publishing** - Redis notification for downstream services

### Performance
- **Max Input Size:** 1MB per request
- **Processing Time:** <100ms for typical content
- **Chunking:** 4000 char max chunks with 400 char overlap
- **Memory Usage:** Minimal footprint with streaming processing

## üîß Configuration

### Environment Variables
```bash
REDIS_URL=redis://localhost:6379/0
EVENT_CHANNEL=idea.events
MAX_INPUT_SIZE=1048576  # 1MB
```

### Service Discovery
- **Container Name:** `idea_db_pre_processor`
- **Network:** `ai_platform`
- **Port:** `3006:8000`
- **Health Check:** `curl http://localhost:3006/health`

## üö® Important Notes for Developers

### ‚ö†Ô∏è DO NOT DUPLICATE FUNCTIONALITY
This service is the **SINGLE SOURCE OF TRUTH** for text normalization. Do not implement HTML‚Üímarkdown conversion in other services. Instead:

1. **Use this service** via HTTP API for all text processing
2. **Content Extractor** should call `/normalize` for HTML/text files
3. **Email Processor** should call `/normalize` for email content
4. **Other services** should integrate via the API, not duplicate logic

### üîÑ Integration Pattern Example
```python
# ‚úÖ CORRECT - Use Pre-Processor API
async def process_html_content(html_content, idea_id):
    async with httpx.AsyncClient() as client:
        response = await client.post(
            "http://idea_db_pre_processor:8000/normalize",
            json={
                "idea_id": idea_id,
                "source": "content_extractor", 
                "payload": html_content
            }
        )
        return response.json()["markdown"]

# ‚ùå WRONG - Don't duplicate markdownify
from markdownify import markdownify  # Don't do this!
```

## üß™ Testing

### Manual Testing
```bash
# Test normalization
curl -X POST http://localhost:3006/normalize \
  -H "Content-Type: application/json" \
  -d '{"idea_id": "test", "source": "manual", "payload": "<h1>Test</h1><p>Content</p>"}'

# Test chunking  
curl -X POST http://localhost:3006/chunk \
  -H "Content-Type: application/json" \
  -d '{"idea_id": "test", "markdown": "# Test\n\nLong content..."}'
```

### Integration Testing
Verify integration with Content Extractor by uploading HTML files through the manual processing API.

## üìö Documentation References

- **Service Architecture:** [DOCKER_ARCHITECTURE.md](../../../docs/DOCKER_ARCHITECTURE.md)
- **API Integration:** [TECHNICAL_SPECS.md](../../../docs/TECHNICAL_SPECS.md)  
- **Content Processing:** Content Extractor service documentation

---

**Last Updated:** 2025-07-17  
**Status:** ‚úÖ Production Ready - Fully Functional  
**Next Steps:** Monitor integration usage and optimize performance as needed 